{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "                               Boosting Techniques\n",
        "\n",
        "1.What is Boosting in Machine Learning?\n",
        "\n",
        ". Boosting in machine learning is an ensemble technique that combines multiple weak learners, typically decision trees, sequentially to create a stronger overall model by focusing more on the errors of previous models.\n",
        "\n",
        "2.How does Boosting differ from Bagging?\n",
        "\n",
        ". Boosting builds models sequentially, each correcting the errors of the previous one, while bagging builds models independently in parallel and combines their results by averaging or voting.\n",
        "\n",
        "3.What is the key idea behind AdaBoost?\n",
        "\n",
        ". The key idea behind AdaBoost is to iteratively adjust the weights of training samples, giving more focus to misclassified examples so that subsequent weak learners improve on the hardest cases.\n",
        "\n",
        "4.Explain the working of AdaBoost with an example.\n",
        "\n",
        ". AdaBoost works by training weak learners sequentially. Initially, all data points have equal weights. After each learner is trained, AdaBoost increases the weights of misclassified points so the next learner focuses more on them. This process repeats, and the final model combines all learners weighted by their accuracy.\n",
        "\n",
        "5.What is Gradient Boosting, and how is it different from AdaBoost?\n",
        "\n",
        ". Gradient Boosting builds models sequentially by optimizing a loss function using gradient descent, fitting each new model to the residual errors of the previous one. Unlike AdaBoost, which adjusts sample weights, Gradient Boosting directly minimizes prediction errors via gradients.\n",
        "\n",
        "6.What is the loss function in Gradient Boosting?\n",
        "\n",
        ". The loss function in Gradient Boosting measures how well the modelâ€™s predictions match the actual values, guiding the model to minimize errors during training by fitting new learners to the negative gradients of this loss.\n",
        "\n",
        "7.How does XGBoost improve over traditional Gradient Boosting?\n",
        "\n",
        ". XGBoost improves traditional Gradient Boosting by adding regularization, handling missing data, using parallel processing, and optimizing speed and memory efficiency for better performance and scalability.\n",
        "\n",
        "8.What is the difference between XGBoost and CatBoost?\n",
        "\n",
        ". XGBoost focuses on speed and regularization, while CatBoost is designed to handle categorical features automatically and reduce prediction shift, making it easier to use with categorical data.\n",
        "\n",
        "9.What are some real-world applications of Boosting techniques?\n",
        "\n",
        ". Boosting techniques are used in fraud detection, spam filtering, customer churn prediction, image recognition, and recommendation systems.\n",
        "\n",
        "10.How does regularization help in XGBoost?\n",
        "\n",
        ". Regularization in XGBoost helps prevent overfitting by penalizing complex models, encouraging simpler trees and improving generalization to new data.\n",
        "\n",
        "11.What are some hyperparameters to tune in Gradient Boosting models?\n",
        "\n",
        ". Key hyperparameters in Gradient Boosting include:\n",
        "\n",
        "Learning rate: controls step size in updates\n",
        "\n",
        "Number of estimators: total trees to build\n",
        "\n",
        "Max depth: limits tree depth\n",
        "\n",
        "Subsample: fraction of data used per tree\n",
        "\n",
        "Min samples split/leaf: controls node splitting\n",
        "\n",
        "Loss function: defines error to minimize\n",
        "\n",
        "12.What is the concept of Feature Importance in Boosting?\n",
        "\n",
        ". Feature importance in Boosting shows how much each feature contributes to the model's predictions, helping identify which features are most influential in decision-making.\n",
        "\n",
        "13.Why is CatBoost efficient for categorical data?\n",
        "\n",
        ". CatBoost is efficient for categorical data because it automatically handles categorical features using a technique called ordered target statistics, reducing the need for manual preprocessing and preventing overfitting.\n",
        "\n",
        "\n",
        "                             Practical\n",
        "\n",
        "14.Train an AdaBoost Classifier on a sample dataset and print model accuracy.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "eHHc5I0A-xJy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Create a sample classification dataset\n",
        "X, y = make_classification(n_samples=1000, n_features=20, n_informative=15,\n",
        "                           n_redundant=5, random_state=42)\n",
        "\n",
        "# Split into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Initialize and train AdaBoost classifier\n",
        "model = AdaBoostClassifier(n_estimators=100, random_state=42)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Predict and evaluate accuracy\n",
        "y_pred = model.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(\"Model Accuracy:\", accuracy)\n"
      ],
      "metadata": {
        "id": "9L_q3lsXGgTn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "15.Train an AdaBoost Regressor and evaluate performance using Mean Absolute Error (MAE)"
      ],
      "metadata": {
        "id": "0lCQmnx2GhUL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "tz0DZxSyHHAw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import AdaBoostRegressor\n",
        "from sklearn.datasets import make_regression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "\n",
        "# Create a sample regression dataset\n",
        "X, y = make_regression(n_samples=1000, n_features=20, noise=0.3, random_state=42)\n",
        "\n",
        "# Split into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Initialize and train AdaBoost regressor\n",
        "regressor = AdaBoostRegressor(n_estimators=100, random_state=42)\n",
        "regressor.fit(X_train, y_train)\n",
        "\n",
        "# Predict and evaluate using Mean Absolute Error\n",
        "y_pred = regressor.predict(X_test)\n",
        "mae = mean_absolute_error(y_test, y_pred)\n",
        "print(\"Mean Absolute Error (MAE):\", mae)\n"
      ],
      "metadata": {
        "id": "hOJjjwjvHGgL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "16.Train a Gradient Boosting Classifier on the Breast Cancer dataset and print feature importance"
      ],
      "metadata": {
        "id": "24jBlabRHKVv"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xedwz3vV-tuE"
      },
      "outputs": [],
      "source": [
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Load the breast cancer dataset\n",
        "data = load_breast_cancer()\n",
        "X = data.data\n",
        "y = data.target\n",
        "feature_names = data.feature_names\n",
        "\n",
        "# Split into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train the Gradient Boosting Classifier\n",
        "model = GradientBoostingClassifier(n_estimators=100, random_state=42)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Get and display feature importances\n",
        "importances = model.feature_importances_\n",
        "feature_importance_df = pd.DataFrame({\n",
        "    'Feature': feature_names,\n",
        "    'Importance': importances\n",
        "}).sort_values(by='Importance', ascending=False)\n",
        "\n",
        "print(feature_importance_df)\n",
        "\n",
        "# Optional: Plot feature importances\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.barh(feature_importance_df['Feature'], feature_importance_df['Importance'])\n",
        "plt.xlabel(\"Feature Importance\")\n",
        "plt.ylabel(\"Features\")\n",
        "plt.title(\"Gradient Boosting Classifier - Feature Importance\")\n",
        "plt.gca().invert_yaxis()\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "17.Train a Gradient Boosting Regressor and evaluate using R-Squared Score"
      ],
      "metadata": {
        "id": "6RO8t9FYHVCI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import GradientBoostingRegressor\n",
        "from sklearn.datasets import make_regression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import r2_score\n",
        "\n",
        "# Create a sample regression dataset\n",
        "X, y = make_regression(n_samples=1000, n_features=20, noise=0.3, random_state=42)\n",
        "\n",
        "# Split into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train the Gradient Boosting Regressor\n",
        "regressor = GradientBoostingRegressor(n_estimators=100, random_state=42)\n",
        "regressor.fit(X_train, y_train)\n",
        "\n",
        "# Predict and evaluate using R-Squared Score\n",
        "y_pred = regressor.predict(X_test)\n",
        "r2 = r2_score(y_test, y_pred)\n",
        "print(\"R-Squared Score:\", r2)\n"
      ],
      "metadata": {
        "id": "9rhRYFdVHh2c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "18.4 Train an XGBoost Classifier on a dataset and compare accuracy with Gradient Boosting"
      ],
      "metadata": {
        "id": "V-c_qwnFHiTX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Create a sample classification dataset\n",
        "X, y = make_classification(n_samples=1000, n_features=20, n_informative=15,\n",
        "                           n_redundant=5, random_state=42)\n",
        "\n",
        "# Split into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train Gradient Boosting Classifier\n",
        "gb_model = GradientBoostingClassifier(n_estimators=100, random_state=42)\n",
        "gb_model.fit(X_train, y_train)\n",
        "gb_preds = gb_model.predict(X_test)\n",
        "gb_accuracy = accuracy_score(y_test, gb_preds)\n",
        "\n",
        "# Train XGBoost Classifier\n",
        "xgb_model = XGBClassifier(use_label_encoder=False, eval_metric='logloss', random_state=42)\n",
        "xgb_model.fit(X_train, y_train)\n",
        "xgb_preds = xgb_model.predict(X_test)\n",
        "xgb_accuracy = accuracy_score(y_test, xgb_preds)\n",
        "\n",
        "# Print accuracies\n",
        "print(f\"Gradient Boosting Accuracy: {gb_accuracy:.4f}\")\n",
        "print(f\"XGBoost Accuracy: {xgb_accuracy:.4f}\")\n"
      ],
      "metadata": {
        "id": "Pu0_1vVNHl7Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "19.Train a CatBoost Classifier and evaluate using F1-Score"
      ],
      "metadata": {
        "id": "Gak4a1t5Huym"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from catboost import CatBoostClassifier\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import f1_score\n",
        "\n",
        "# Create a sample classification dataset\n",
        "X, y = make_classification(n_samples=1000, n_features=20, n_informative=15,\n",
        "                           n_redundant=5, random_state=42)\n",
        "\n",
        "# Split into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Initialize and train CatBoost Classifier\n",
        "model = CatBoostClassifier(iterations=100, random_seed=42, verbose=0)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Predict on test set\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Evaluate using F1-Score\n",
        "f1 = f1_score(y_test, y_pred)\n",
        "print(\"F1-Score:\", f1)\n"
      ],
      "metadata": {
        "id": "zzVv3mQ5HxFu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "20.Train an XGBoost Regressor and evaluate using Mean Squared Error (MSE)"
      ],
      "metadata": {
        "id": "L48lnUoeH731"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from xgboost import XGBRegressor\n",
        "from sklearn.datasets import make_regression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# Create a sample regression dataset\n",
        "X, y = make_regression(n_samples=1000, n_features=20, noise=0.3, random_state=42)\n",
        "\n",
        "# Split into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Initialize and train XGBoost Regressor\n",
        "model = XGBRegressor(n_estimators=100, random_state=42)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Predict on test set\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Evaluate using Mean Squared Error\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "print(\"Mean Squared Error (MSE):\", mse)\n"
      ],
      "metadata": {
        "id": "uK56TZ99IAgN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "21.Train an AdaBoost Classifier and visualize feature importance"
      ],
      "metadata": {
        "id": "6dP6NYN_JKOw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import train_test_split\n",
        "import numpy as np\n",
        "\n",
        "# Create a sample classification dataset\n",
        "X, y = make_classification(n_samples=1000, n_features=20, n_informative=15,\n",
        "                           n_redundant=5, random_state=42)\n",
        "\n",
        "# Split into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train AdaBoost classifier\n",
        "model = AdaBoostClassifier(n_estimators=100, random_state=42)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Get feature importances\n",
        "importances = model.feature_importances_\n",
        "features = np.arange(len(importances))\n",
        "\n",
        "# Plot feature importances\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.barh(features, importances, color='skyblue')\n",
        "plt.xlabel('Feature Importance')\n",
        "plt.ylabel('Feature Index')\n",
        "plt.title('AdaBoost Classifier Feature Importance')\n",
        "plt.gca().invert_yaxis()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "dtKbh6tBJP1W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "22.Train a Gradient Boosting Regressor and plot learning curves."
      ],
      "metadata": {
        "id": "3cTDaDqXJUC4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.ensemble import GradientBoostingRegressor\n",
        "from sklearn.datasets import make_regression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# Create a sample regression dataset\n",
        "X, y = make_regression(n_samples=1000, n_features=20, noise=0.3, random_state=42)\n",
        "\n",
        "# Split into training and validation sets\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train Gradient Boosting Regressor with staged predictions\n",
        "model = GradientBoostingRegressor(n_estimators=100, random_state=42)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Track training and validation error at each stage\n",
        "train_errors = []\n",
        "val_errors = []\n",
        "\n",
        "for y_train_pred in model.staged_predict(X_train):\n",
        "    train_errors.append(mean_squared_error(y_train, y_train_pred))\n",
        "for y_val_pred in model.staged_predict(X_val):\n",
        "    val_errors.append(mean_squared_error(y_val, y_val_pred))\n",
        "\n",
        "# Plot learning curves\n",
        "plt.figure(figsize=(10,6))\n",
        "plt.plot(train_errors, label='Training MSE')\n",
        "plt.plot(val_errors, label='Validation MSE')\n",
        "plt.xlabel('Number of Trees')\n",
        "plt.ylabel('Mean Squared Error')\n",
        "plt.title('Gradient Boosting Regressor Learning Curves')\n",
        "plt.legend()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "TokoOTkRJYDH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "23. Train an XGBoost Classifier and visualize feature importance."
      ],
      "metadata": {
        "id": "xoZMoU8iJYi5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from xgboost import XGBClassifier, plot_importance\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Create a sample classification dataset\n",
        "X, y = make_classification(n_samples=1000, n_features=20, n_informative=15,\n",
        "                           n_redundant=5, random_state=42)\n",
        "\n",
        "# Split into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train XGBoost Classifier\n",
        "model = XGBClassifier(use_label_encoder=False, eval_metric='logloss', random_state=42)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Plot feature importance\n",
        "plt.figure(figsize=(10, 8))\n",
        "plot_importance(model, max_num_features=20, importance_type='weight', show_values=False)\n",
        "plt.title('XGBoost Feature Importance')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "Ojq70AHGJghV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "24.Train a CatBoost Classifier and plot the confusion matrix."
      ],
      "metadata": {
        "id": "xXByhyTtJg16"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from catboost import CatBoostClassifier\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Create a sample classification dataset\n",
        "X, y = make_classification(n_samples=1000, n_features=20, n_informative=15,\n",
        "                           n_redundant=5, random_state=42)\n",
        "\n",
        "# Split dataset\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train CatBoost Classifier\n",
        "model = CatBoostClassifier(iterations=100, random_seed=42, verbose=0)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Predict on test set\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Compute confusion matrix\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "# Plot confusion matrix\n",
        "disp = ConfusionMatrixDisplay(confusion_matrix=cm)\n",
        "disp.plot(cmap=plt.cm.Blues)\n",
        "plt.title(\"CatBoost Classifier Confusion Matrix\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "K0MoGHb1Jn4b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "25.Train an AdaBoost Classifier with different numbers of estimators and compare accuracy.\n"
      ],
      "metadata": {
        "id": "zr0sefakJo7G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Create a sample classification dataset\n",
        "X, y = make_classification(n_samples=1000, n_features=20, n_informative=15,\n",
        "                           n_redundant=5, random_state=42)\n",
        "\n",
        "# Split into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Different numbers of estimators to try\n",
        "estimators_list = [10, 50, 100, 150, 200]\n",
        "accuracies = []\n",
        "\n",
        "for n_estimators in estimators_list:\n",
        "    model = AdaBoostClassifier(n_estimators=n_estimators, random_state=42)\n",
        "    model.fit(X_train, y_train)\n",
        "    y_pred = model.predict(X_test)\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "    accuracies.append(accuracy)\n",
        "    print(f\"Estimators: {n_estimators} - Accuracy: {accuracy:.4f}\")\n",
        "\n",
        "# Plot accuracy vs number of estimators\n",
        "plt.figure(figsize=(8,5))\n",
        "plt.plot(estimators_list, accuracies, marker='o')\n",
        "plt.xlabel('Number of Estimators')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.title('AdaBoost Accuracy vs Number of Estimators')\n",
        "plt.grid(True)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "V3jHBGPtJ2m4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "26.Train a Gradient Boosting Classifier and visualize the ROC curve.\n"
      ],
      "metadata": {
        "id": "SOhnBassLPUh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import roc_curve, auc\n",
        "\n",
        "# Create a sample classification dataset\n",
        "X, y = make_classification(n_samples=1000, n_features=20, n_informative=15,\n",
        "                           n_redundant=5, random_state=42)\n",
        "\n",
        "# Split into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train Gradient Boosting Classifier\n",
        "model = GradientBoostingClassifier(n_estimators=100, random_state=42)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Predict probabilities for positive class\n",
        "y_scores = model.predict_proba(X_test)[:, 1]\n",
        "\n",
        "# Compute ROC curve and AUC\n",
        "fpr, tpr, thresholds = roc_curve(y_test, y_scores)\n",
        "roc_auc = auc(fpr, tpr)\n",
        "\n",
        "# Plot ROC curve\n",
        "plt.figure(figsize=(8,6))\n",
        "plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (AUC = {roc_auc:.4f})')\n",
        "plt.plot([0,1], [0,1], color='navy', lw=2, linestyle='--')\n",
        "plt.xlim([0.0, 1.0])\n",
        "plt.ylim([0.0, 1.05])\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.title('Gradient Boosting Classifier ROC Curve')\n",
        "plt.legend(loc=\"lower right\")\n",
        "plt.grid(True)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "EGbmh_s1Lavj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "27.Train an XGBoost Regressor and tune the learning rate using GridSearchCV"
      ],
      "metadata": {
        "id": "l0AEjDaGLbQX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from xgboost import XGBRegressor\n",
        "from sklearn.datasets import make_regression\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# Create sample regression dataset\n",
        "X, y = make_regression(n_samples=1000, n_features=20, noise=0.3, random_state=42)\n",
        "\n",
        "# Split into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Initialize XGBoost Regressor\n",
        "xgb = XGBRegressor(n_estimators=100, random_state=42)\n",
        "\n",
        "# Define parameter grid for learning rate\n",
        "param_grid = {\n",
        "    'learning_rate': [0.01, 0.05, 0.1, 0.2]\n",
        "}\n",
        "\n",
        "# Setup GridSearchCV\n",
        "grid_search = GridSearchCV(estimator=xgb, param_grid=param_grid,\n",
        "                           cv=3, scoring='neg_mean_squared_error', verbose=1)\n",
        "\n",
        "# Fit GridSearchCV\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Best learning rate\n",
        "best_lr = grid_search.best_params_['learning_rate']\n",
        "print(f\"Best learning rate: {best_lr}\")\n",
        "\n",
        "# Evaluate on test set with best estimator\n",
        "best_model = grid_search.best_estimator_\n",
        "y_pred = best_model.predict(X_test)\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "print(f\"Test MSE with best learning rate: {mse:.4f}\")\n"
      ],
      "metadata": {
        "id": "AmWKl9g1Lf-c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "28.Train a CatBoost Classifier on an imbalanced dataset and compare performance with class weighting.\n"
      ],
      "metadata": {
        "id": "1SxRsQnRLkes"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from catboost import CatBoostClassifier\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import f1_score\n",
        "import numpy as np\n",
        "\n",
        "# Create an imbalanced classification dataset\n",
        "X, y = make_classification(n_samples=1000, n_features=20, n_informative=10,\n",
        "                           n_redundant=5, weights=[0.9, 0.1], flip_y=0,\n",
        "                           random_state=42)\n",
        "\n",
        "# Split into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train CatBoost without class weights\n",
        "model_no_weight = CatBoostClassifier(iterations=100, random_seed=42, verbose=0)\n",
        "model_no_weight.fit(X_train, y_train)\n",
        "y_pred_no_weight = model_no_weight.predict(X_test)\n",
        "f1_no_weight = f1_score(y_test, y_pred_no_weight)\n",
        "\n",
        "# Compute class weights manually (inverse of class frequency)\n",
        "classes, counts = np.unique(y_train, return_counts=True)\n",
        "class_weights = {cls: 1.0/count for cls, count in zip(classes, counts)}\n",
        "\n",
        "# Map sample weights based on class weights\n",
        "sample_weights = np.array([class_weights[label] for label in y_train])\n",
        "\n",
        "# Train CatBoost with class weights via sample weights\n",
        "model_weighted = CatBoostClassifier(iterations=100, random_seed=42, verbose=0)\n",
        "model_weighted.fit(X_train, y_train, sample_weight=sample_weights)\n",
        "y_pred_weighted = model_weighted.predict(X_test)\n",
        "f1_weighted = f1_score(y_test, y_pred_weighted)\n",
        "\n",
        "print(f\"F1-Score without class weighting: {f1_no_weight:.4f}\")\n",
        "print(f\"F1-Score with class weighting: {f1_weighted:.4f}\")\n"
      ],
      "metadata": {
        "id": "jMkiOIIWLoYv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "29.Train an AdaBoost Classifier and analyze the effect of different learning rates.\n"
      ],
      "metadata": {
        "id": "Lt09MydlLo6F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Create a sample classification dataset\n",
        "X, y = make_classification(n_samples=1000, n_features=20, n_informative=15,\n",
        "                           n_redundant=5, random_state=42)\n",
        "\n",
        "# Split dataset\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Different learning rates to test\n",
        "learning_rates = [0.01, 0.05, 0.1, 0.2, 0.5, 1.0]\n",
        "accuracies = []\n",
        "\n",
        "for lr in learning_rates:\n",
        "    model = AdaBoostClassifier(n_estimators=100, learning_rate=lr, random_state=42)\n",
        "    model.fit(X_train, y_train)\n",
        "    y_pred = model.predict(X_test)\n",
        "    acc = accuracy_score(y_test, y_pred)\n",
        "    accuracies.append(acc)\n",
        "    print(f\"Learning Rate: {lr} - Accuracy: {acc:.4f}\")\n",
        "\n",
        "# Plot accuracy vs learning rate\n",
        "plt.figure(figsize=(8,5))\n",
        "plt.plot(learning_rates, accuracies, marker='o')\n",
        "plt.xlabel('Learning Rate')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.title('Effect of Learning Rate on AdaBoost Accuracy')\n",
        "plt.grid(True)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "EYAg1vSGLwDX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "30.Train an XGBoost Classifier for multi-class classification and evaluate using log-loss.\n"
      ],
      "metadata": {
        "id": "terNaufBLwpH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from xgboost import XGBClassifier\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import log_loss\n",
        "\n",
        "# Create a sample multi-class dataset (e.g., 3 classes)\n",
        "X, y = make_classification(n_samples=1000, n_features=20, n_informative=15,\n",
        "                           n_redundant=5, n_classes=3, random_state=42)\n",
        "\n",
        "# Split dataset\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Initialize and train XGBoost Classifier for multi-class\n",
        "model = XGBClassifier(objective='multi:softprob', num_class=3,\n",
        "                      use_label_encoder=False, eval_metric='mlogloss', random_state=42)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Predict probabilities on test set\n",
        "y_proba = model.predict_proba(X_test)\n",
        "\n",
        "# Calculate log-loss\n",
        "logloss = log_loss(y_test, y_proba)\n",
        "print(f\"Multi-class Log-Loss: {logloss:.4f}\")\n"
      ],
      "metadata": {
        "id": "cfErIYUrL2cP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "rHaFE3oNH_sH"
      }
    }
  ]
}